services:
  # Base de données TimescaleDB
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: cryptoviz-timescaledb
    environment:
      POSTGRES_DB: ${TIMESCALE_DB:-cryptoviz}
      POSTGRES_USER: ${TIMESCALE_USER:-postgres}
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD:-password}
      POSTGRES_HOST_AUTH_METHOD: trust
    # Memory limits to prevent unbounded growth during tiering operations
    shm_size: 512m             # Shared memory for PostgreSQL internal operations
    ports:
      - "7432:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      # Database initialization scripts (executed ONLY on first startup when volume is empty)
      # NOTE: If the database already exists, these scripts will NOT re-run
      # To force re-initialization: use 'make db-reset' or 'make clean && make start'
      - ./database/init.sql:/docker-entrypoint-initdb.d/01-init.sql
      - ./database/setup-tiering.sql:/docker-entrypoint-initdb.d/02-setup-tiering.sql
      - ./database/setup-indicators.sql:/docker-entrypoint-initdb.d/03-setup-indicators.sql
      - ./database/04-backfill-tracking.sql:/docker-entrypoint-initdb.d/04-backfill-tracking.sql
    networks:
      - cryptoviz-network
    restart: unless-stopped
    # PostgreSQL Memory Tuning (optimized for 6GB memory limit)
    # shared_buffers: 25% of mem_limit (1536MB of 6GB)
    # work_mem: per-operation memory for sorts/hashes (32MB allows ~20 concurrent ops)
    # maintenance_work_mem: for DELETE/VACUUM/INDEX operations (512MB for large tiering ops)
    command: >
      postgres
      -c shared_buffers=1536MB
      -c work_mem=32MB
      -c maintenance_work_mem=512MB
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${TIMESCALE_USER:-postgres} -d ${TIMESCALE_DB:-cryptoviz}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s  # Grace period for database initialization scripts (init.sql, setup-tiering.sql)

  # Zookeeper pour Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: cryptoviz-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - cryptoviz-network
    restart: unless-stopped


  # Kafka UI (explorateur de topics)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: cryptoviz-kafka-ui
    depends_on:
      - kafka
      - zookeeper
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      # Optionnel mais utile si ZK est présent
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    ports:
      - "8082:8080"
    networks:
      - cryptoviz-network
    restart: unless-stopped

  # Apache Kafka
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: cryptoviz-kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_NUM_PARTITIONS: 3
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Grace period for Kafka broker initialization and consumer offsets loading

  # Redis (optionnel pour cache)
  redis:
    image: redis:7-alpine
    container_name: cryptoviz-redis
    ports:
      - "7379:6379"
    volumes:
      - redis_data:/data
    networks:
      - cryptoviz-network
    restart: unless-stopped
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MinIO - S3-compatible object storage for data tiering
  minio:
    image: minio/minio:latest
    container_name: cryptoviz-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"      # API
      - "9001:9001"      # Console UI
    volumes:
      - minio_data:/data
    networks:
      - cryptoviz-network
    command: server /data --console-address ":9001"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MinIO Client - Initialize buckets for tiering
  minio-init:
    image: minio/mc:latest
    container_name: cryptoviz-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb myminio/cryptoviz-cold-storage --ignore-existing;
      /usr/bin/mc anonymous set download myminio/cryptoviz-cold-storage;
      echo 'MinIO bucket created: cryptoviz-cold-storage';
      exit 0;
      "
    networks:
      - cryptoviz-network
    restart: "no"  # One-time init container - only runs once to create bucket

  # Data Collector Service
  data-collector:
    build:
      context: ./services/data-collector
      dockerfile: Dockerfile
    container_name: cryptoviz-data-collector
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
    environment:
      # Binance API
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}

      # Kafka
      KAFKA_BROKERS: kafka:29092

      # TimescaleDB
      TIMESCALE_HOST: timescaledb
      TIMESCALE_PORT: 5432
      TIMESCALE_DB: ${TIMESCALE_DB:-cryptoviz}
      TIMESCALE_USER: ${TIMESCALE_USER:-postgres}
      TIMESCALE_PASSWORD: ${TIMESCALE_PASSWORD:-password}

      # Data Collector - Filtrage des symboles
      QUOTE_CURRENCIES: ${QUOTE_CURRENCIES:-USDT,BUSD,FDUSD}
      MIN_VOLUME: ${MIN_VOLUME:-5000000}
      MAX_SYMBOLS: ${MAX_SYMBOLS:-20}

      # Data Collector - Features
      ENABLE_TRADES: ${ENABLE_TRADES:-true}
      ENABLE_TICKER: ${ENABLE_TICKER:-false}
      ENABLE_AGGREGATION: ${ENABLE_AGGREGATION:-true}

      # Data Collector - Backfill automatique
      ENABLE_BACKFILL: ${ENABLE_BACKFILL:-true}
      BACKFILL_LOOKBACK_DAYS: ${BACKFILL_LOOKBACK_DAYS:-365}
      BACKFILL_TIMEFRAMES: ${BACKFILL_TIMEFRAMES:-1m,5m,15m,1h,1d}
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'python.*main.py' > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Grace period for Kafka connection + Binance API fetching
    volumes:
      - ./services/data-collector:/app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # News Scraper Service
  news-scraper:
    build:
      context: ./services/news-scraper
      dockerfile: Dockerfile
    container_name: cryptoviz-news-scraper
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BROKERS: kafka:29092
      SCRAPING_INTERVAL: ${SCRAPING_INTERVAL:-300}
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'python.*app.py' > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s  # Grace period for Kafka connection and initial scraping setup
    volumes:
      - ./services/news-scraper:/app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Backend Go Service
  backend-go:
    build:
      context: ./services/backend-go
      dockerfile: Dockerfile
    container_name: cryptoviz-backend-go
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      PORT: ${BACKEND_PORT:-8080}
      KAFKA_BROKERS: kafka:29092
      TIMESCALE_HOST: timescaledb
      TIMESCALE_PORT: 5432
      TIMESCALE_DB: ${TIMESCALE_DB:-cryptoviz}
      TIMESCALE_USER: ${TIMESCALE_USER:-postgres}
      TIMESCALE_PASSWORD: ${TIMESCALE_PASSWORD:-password}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      GIN_MODE: ${GIN_MODE:-release}
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f './main' > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Grace period for database/Kafka/Redis connections and WebSocket hub initialization
    volumes:
      - ./services/backend-go:/app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Frontend Vue.js Service
  frontend-vue:
    build:
      context: ./services/frontend-vue
      dockerfile: Dockerfile
      args:
        - VITE_API_URL=${VITE_API_URL:-http://localhost:8080}
        - VITE_WS_URL=${VITE_WS_URL:-ws://localhost:8080/ws/crypto}
        - VITE_USE_MOCK=${VITE_USE_MOCK:-false}
    container_name: cryptoviz-frontend-vue
    depends_on:
      - backend-go
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    networks:
      - cryptoviz-network
    restart: unless-stopped
    volumes:
      - ./services/frontend-vue:/app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Kafka Topics Initializer
  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    container_name: cryptoviz-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    command: |
      bash -c "
        set -e
        echo 'Création des topics Kafka...'
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.raw.trades  --config cleanup.policy=delete --config retention.ms=172800000

        # TICKER (snapshot, compactable)
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.ticker      --config cleanup.policy=compact,delete --config retention.ms=259200000

        # AGGREGATED OHLCV (compactable) - Intervals: 1m, 5m, 15m, 1h, 1d
        # Fast compaction settings to prevent duplicate accumulation:
        # - segment.ms=3600000 (1h) - Close segments hourly instead of default 7 days
        # - min.cleanable.dirty.ratio=0.5 - Compact when 50% of log is dirty (duplicates)
        # - max.compaction.lag.ms=3600000 (1h) - Force compaction at least once per hour
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.aggregated.1m  --config cleanup.policy=compact,delete --config retention.ms=2592000000 --config segment.ms=3600000 --config min.cleanable.dirty.ratio=0.5 --config max.compaction.lag.ms=3600000
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.aggregated.5m  --config cleanup.policy=compact,delete --config retention.ms=5184000000 --config segment.ms=3600000 --config min.cleanable.dirty.ratio=0.5 --config max.compaction.lag.ms=3600000
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.aggregated.15m --config cleanup.policy=compact,delete --config retention.ms=7776000000 --config segment.ms=3600000 --config min.cleanable.dirty.ratio=0.5 --config max.compaction.lag.ms=3600000
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.aggregated.1h  --config cleanup.policy=compact,delete --config retention.ms=15552000000 --config segment.ms=3600000 --config min.cleanable.dirty.ratio=0.5 --config max.compaction.lag.ms=3600000
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.aggregated.1d  --config cleanup.policy=compact,delete --config retention.ms=63072000000 --config segment.ms=3600000 --config min.cleanable.dirty.ratio=0.5 --config max.compaction.lag.ms=3600000

        # NEWS (peu critique)
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic crypto.news         --config cleanup.policy=delete --config retention.ms=604800000

        echo 'Topics Kafka créés avec succès!'
        kafka-topics --list --bootstrap-server kafka:29092
      "
    networks:
      - cryptoviz-network
    restart: "no"  # One-time init container - only runs once to create topics

  # ========== MONITORING STACK ==========

  # Prometheus - Metrics Collection
  prometheus:
    image: prom/prometheus:latest
    container_name: cryptoviz-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana - Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: cryptoviz-grafana
    depends_on:
      - prometheus
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: 'false'
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3001}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Node Exporter - System Metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: cryptoviz-node-exporter
    command:
      - '--path.rootfs=/host'
    ports:
      - "9100:9100"
    volumes:
      - /:/host:ro,rslave
    networks:
      - cryptoviz-network
    restart: unless-stopped

  # cAdvisor - Container Metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cryptoviz-cadvisor
    ports:
      - "8083:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks:
      - cryptoviz-network
    restart: unless-stopped
    privileged: true
    devices:
      - /dev/kmsg

  # PostgreSQL Exporter - TimescaleDB Metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: cryptoviz-postgres-exporter
    depends_on:
      timescaledb:
        condition: service_healthy
    environment:
      DATA_SOURCE_NAME: "postgresql://${TIMESCALE_USER:-postgres}:${TIMESCALE_PASSWORD:-password}@timescaledb:5432/${TIMESCALE_DB:-cryptoviz}?sslmode=disable"
    ports:
      - "9187:9187"
    networks:
      - cryptoviz-network
    restart: unless-stopped

  # Redis Exporter - Redis Metrics
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: cryptoviz-redis-exporter
    depends_on:
      redis:
        condition: service_healthy
    environment:
      REDIS_ADDR: redis:6379
    ports:
      - "9121:9121"
    networks:
      - cryptoviz-network
    restart: unless-stopped

  # Kafka Exporter - Kafka Metrics
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: cryptoviz-kafka-exporter
    depends_on:
      kafka:
        condition: service_healthy
    command:
      - '--kafka.server=kafka:29092'
    ports:
      - "9308:9308"
    networks:
      - cryptoviz-network
    restart: unless-stopped

  # Gatus - Health Check & Status Page
  gatus:
    image: twinproduction/gatus:latest
    container_name: cryptoviz-gatus
    depends_on:
      - timescaledb
      - kafka
      - redis
      - backend-go
    ports:
      - "8084:8080"
    volumes:
      - ./monitoring/gatus/config.yaml:/config/config.yaml
      - gatus_data:/data
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Indicators Scheduler Service
  # Remplace les TimescaleDB jobs (bug "cache lookup failed")
  indicators-scheduler:
    build:
      context: ./services/indicators-scheduler
      dockerfile: Dockerfile
    container_name: cryptoviz-indicators-scheduler
    environment:
      PGHOST: timescaledb
      PGPORT: 5432
      PGDATABASE: cryptoviz
      PGUSER: postgres
      PGPASSWORD: ${POSTGRES_PASSWORD:-postgres}
    networks:
      - cryptoviz-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'python.*scheduler.py' > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s  # Grace period for database connection and scheduler initialization
    depends_on:
      timescaledb:
        condition: service_healthy
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  timescaledb_data:
    driver: local
  kafka_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_logs:
    driver: local
  redis_data:
    driver: local
  minio_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  gatus_data:
    driver: local

networks:
  cryptoviz-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
